# Notes on 2 February 2021

## Agenda

- FastAI.jl updates from Lorenz
- [Flux Publish theme][1]
- Metalhead.jl updates
	- ONNX2Flux.jl  ?
- Optimisers.jl [updates][2]
	- Demo with ParameterSchedulers.jl

## Minutes

**Video link:** [https://youtu.be/140pvNLDWCU][3]

### FastAI.jl updates

_Goal:_
Consolidate work on various packages from last year into FastAI.jl. The package isn’t necessarily a strict clone of Python’s fast.ai, but it takes heavy inspiration from it.

Some of the major packages that make up FastAI.jl:
- FluxTraining.jl: provides the training loop
- DataAugmentation.jl: provides data augmentation for 2D/3D images
- DLPipelines.jl: interfaces to define ML problems/tasks at a high level

Lorenz showed [a demo][4] on Zulip. Folks can follow minor development updates in that thread.

Next steps: Lorenz will do some clean-up to make the repo clone-able. We want to have a working main branch as soon as possible so that we can have an issue/PR cycle going.

### Metalhead.jl updates

Currently, the only blocker is pre-trained weights for some models that had them in the old Metalhead.jl. Originally, Kyle was trying to train on a GPU cluster but getting the hyperparameters right for ImageNet is proving difficult.

New plan:
- Port old pre-trained weights to new models
- Long-term look into ONNX2Flux conversion (Dhairya might have some scripts to help with this)

Lorenz is looking for something like ResNet-18 to be available for FastAI.jl. Eventually going through the ImageNet training will be a useful test case of the packages.

### Optimisers.jl updates

Optimizers are moved out of Flux into Optimisers.jl. Each optimizer now has explicit state and immutability.

There are some concerns about what to rename `Optimiser`. Currently it is `SequenceOptimiser`, but we agreed to `ChainOptimiser` in the end. There were some thoughts around using `Chain` for this:
- PyTorch does something similar with transforms and `nn.Sequential`
- We decided against it because it complicates the semantics

### Data parallel discussion

Current data parallel approach with `pmap` results in OOM errors (on Metalhead PR). It also lacks an all-reduce operation on the GPU-side to collect the gradient updates. A couple of updates on this front:
- [Recent PR to CUDA.jl][5] can allow better semantics w.r.t. data loading
- Valentin is working on UCX support
	- We think MPI is written on top of UCX?
	- Should address the data parallel all-reduce issue
	- Higher-level semantics on top of UCX with Dagger.jl

[1]:	https://github.com/darsnack/flux-theme
[2]:	https://github.com/FluxML/Optimisers.jl/pull/9
[3]:	https://youtu.be/140pvNLDWCU
[4]:	https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/FastAI.2Ejl.20development/near/224081840
[5]:	https://github.com/JuliaGPU/CUDA.jl/pull/662