# Notes on 1 Feb. 2022

## Agenda

- `Embedding` changes (see [PR](https://github.com/FluxML/Flux.jl/pull/1656))

## Minutes

**Video link:** https://youtu.be/jeYpqPPCuZY

A summary of the discussion on the `Embedding` PR can be found [here](https://github.com/FluxML/Flux.jl/pull/1656#issuecomment-1027151130).

Lorenz gave an update demo on his new documentation system which now features structured+indexed search, navigation, and mobile support.

We discussed [MLUtils.jl](https://github.com/JuliaML/MLUtils.jl). If you are interested in helping shape the future of MLDataPattern.jl or the data pipeline story in Julia, please consider contributing to that repo.

Dhairya gave a demo of his wandb project on data parallel training. The output of this effort will help add pre-trained model weights to Metalhead.jl.

Kyle gave an update on Optimisers.jl. The current work is focused on ironing out edge cases like tied weights with explicit gradients.

### Chat transcript

```
18:04:25	 From Lorenz Ohly : https://github.com/FluxML/ML-Coordination-Tracker/blob/main/meeting-minutes/2022-02-01.md
18:05:37	 From Lorenz Ohly : PR: https://github.com/FluxML/Flux.jl/pull/1656
18:06:40	 From Lorenz Ohly : https://github.com/FluxML/Flux.jl/pull/1830
18:07:22	 From Kyle Daruwalla : I think the issue is that x passed in could be a vector or matrix and the vector case is indexing
18:07:45	 From Kyle Daruwalla : Outputsize wonâ€™t behave well in an index operation
18:08:38	 From Brian Chen : could we always reshape in the vector case? would that help?
18:08:39	 From Kyle Daruwalla : So maybe we define a manual outputsize rule for Embedding + vector
18:08:56	 From Kyle Daruwalla : I think reshaping will only hit a fast path if it is one-hot
18:09:51	 From Kyle Daruwalla : I think that PR will only help hit the right path
18:10:04	 From Brian Chen : yeah there are a couple of cases where reshaped OH Arrays will dispatch to the fast path, but I don't recall which
18:10:20	 From Kyle Daruwalla : Michaelâ€™s PR will make sure we only use Nil when the array is an array of reals
18:10:28	 From Kyle Daruwalla : Which is part of the issue here
18:12:08	 From Kyle Daruwalla : Menubar looks like it would help a lot!
18:12:19	 From Brian Chen : Seconded ^^^
18:13:39	 From Kyle Daruwalla : Nice!
18:13:46	 From Brian Chen : Structured search!
18:14:15	 From Brian Chen : is the input format the same as Publish.jl or is it custom?
18:14:41	 From Brian Chen : very nice
18:15:02	 From Brian Chen : even nicer!
18:15:54	 From Kyle Daruwalla : One other thing I want to mention is MLUtils.jl. Any one interested in future LearnBase/MLDataPattern.jl stuff should watch Carloâ€™s work there and give their input.
18:15:55	 From Andrew Dinhobl : Looks very cool!
18:15:56	 From Brian Chen : ship it haha
18:16:09	 From Kyle Daruwalla : Just want to make that announcement
18:16:26	 From Lorenz Ohly : https://github.com/JuliaML/MLUtils.jl
18:17:10	 From Dhairya Gandhi : https://wandb.ai/dhairyalgandhi/DDP_Trantor?workspace=

pipeline for training large models with data parallelism ^
18:17:12	 From Kyle Daruwalla : Deprecated in a stable-frozen way
18:19:50	 From Kyle Daruwalla : Iâ€™ve used it with other threading before
18:20:11	 From Brian Chen : pipeline parallelism is normal parallelism + microbatches
18:20:33	 From Brian Chen : So I think it's more of a training thing rather than a data loading one
18:21:48	 From Kyle Daruwalla : Is this based on ResNetImageNet?
18:22:25	 From Kyle Daruwalla : Cool! So is this is being used to pretrain Metalhead stuff?
18:23:22	 From Kyle Daruwalla : Great progress
18:24:16	 From Kyle Daruwalla : Brian can you unmute? I can if I have to but I am in public.
18:24:56	 From Brian Chen : I can't either sorry
18:26:06	 From Brian Chen : Overall goal is to figure out all the bits we need to reach parity/fix edge cases with Flux's current optimizers
18:26:40	 From Brian Chen : I can't either ðŸ˜‚
18:27:34	 From Brian Chen : They will likely require some testing before we know which is "right"
18:28:59	 From Brian Chen : and there's already a precedent for it in JAX land
18:31:06	 From Brian Chen : maybe an action to run manually before every release?
18:31:15	 From Lorenz Ohly : Good idea
18:33:04	 From Brian Chen : thanks all
```