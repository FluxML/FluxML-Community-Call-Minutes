# Notes on 22 Sept. 2023

## Agenda

- Llama 2 discussion
- Memory allocation discussions around escape analysis (EA) improvements

## Minutes

Llama 2 discussion:
- How to organize development around Llama 2 and LLMs in Flux / Julia
- Julian has been working on Llama2.jl to save/load various saved model formats
- Transformers.jl can load Llama 2 from HuggingFace
    - This does not support all the formats that Llama2.jl does
    - Lacks similar feature set
    - Code is more difficult to follow
- Julian would like to do distributed training with Llama 2, but Llama2.jl is not in Flux, so can't use DaggerFlux.jl
    - Too much work to make Dagger.jl work on Llama2.jl (lots of reimplementation)
- Easiest starting point is a single .jl file implementation of Llama 2 using latest Flux layers (MHA, etc.)
- Julian will continue working on Llama2.jl for now (GGML format updates)

Memory allocation discussion:
- Julian has some code to memory map allocations in arbitrary code with a context `with_mmap(...) do ... end`
- Doesn't have to be file backed memory
- Idea: temporary allocations in Flux get mmaped once then reserve those on the next pass of the model
    - Relies on allocation ordering be fixed
- Reducing peak memory usage vs. reducing memory thrashing
    - We have higher peak memory usage b/c our AD assumes that all temporaries need to be held all the way through the backwards pass
    - PyTorch tears down the graph on the backwards pass
