# Notes on 29 September 2020

## Agenda

- Transfer of FluxTraining.jl to FastAI.jl
- Design of DAG system
- Helping Dhairya with some GAN networks
  - Tutorials in general

## Minutes

### Migration of FluxTraining.jl

FluxTraining.jl is a better foundation, so Peter and Lorenz want to merge the two efforts.

**Discussion:** Should FluxTraining.jl merge with FastAI.jl, or should FastAI.jl depend on FluxTraining.jl?

- Do higher level fast.ai APIs belong in FluxTraining.jl?
- Lorenz seemed receptive to FluxTraining.jl stack mirroring FastAI.jl in functionality
- No value in low-level core without high-level AP

**Decision:** Merge both repos (i.e. move FluxTraining.jl code to FastAI.jl) then build full stack.

### Design of DAG System

A DAG (directed acyclic graph) is an emergent property of executing the training loop. Callbacks and standard loop computations are all part of the same DAG.

**Motivating example:** Peter does NLP on bank data. You cannot run on your own machine; a cluster is provided by the customer. It would be useful to execute code on multiple different notions of parallelized hardware. GPUs are not readily available on these clusters and rewriting the code for multi-core CPU execution is not attractive.

Last meeting we discussed using Dagger.jl as an execution framework. Shashi provided a summary of Dagger.jl:
- Framework to delay computation
- Returns a DAG of computation
- There is a `compute(dag)` function that executes the DAG on some parallelized hardware
- Multi-GPU reduction requires copy to CPU
- People working on UCX communications library

To support multiple execution backends/schedulers, we should create an AbstractDAGBase package. Notes on this discussion:
- Single threaded, serial default
- Agnostic of backend
- No need for parallelism out the gate
- What is the timeline?
  - Fast: go for Dagger.jl
  - Slow: take time to come up with AbstractDAG package
- Composability
    - *Other threaded framework within a node on a Dagger DAG*:
        - Dagger runs on processes across multiple machines
        - Other lib runs on threads within single process on a single machine
    - *Dagger DAG within thread/task on other lib*:
        - Status unknown
        - `compute` isn't aware of global task queue
    - Composability can be built later on top of AbstractDAG
    - Generally seems to be a scheduler issue (not an API issue)

Our DAG sounds a lot like Tensorflow's model. This led to a discussion on dynamic vs. static task schedulers. We concluded that there is *no need for dynamic scheduling at the granularity on which our DAG is built*.

### Chat Log

12:07:14	 From Neil Rhodes : I’m Neil: prof at Harvey Mudd College, and interesting in using Julia and fast.ai for a deep learning course in the spring.

12:07:39	 From Brian Chen : Welcome!

12:07:45	 From Kyle Daruwalla : Welcome!

12:08:01	 From Neil Rhodes : Also interested in contributing however I can

12:08:03	 From Ari Katz : Welcome indeed! There are some beginner and ML courses here that you may be able to adapt: https://juliaacademy.com/ 

12:08:26	 From Neil Rhodes : Don’t need a course. Just need fast.ai capabilities in Julia:)

12:08:37	 From Ari Katz : then you've come to the right place :) 

12:10:33	 From Ari Katz : https://github.com/lorenzoh/FluxTraining.jl fyi

12:10:47	 From Ari Katz : https://github.com/FluxML/FastAI.jl

12:21:45	 From Brian Chen : https://github.com/JuliaParallel/Dagger.jl

12:30:17	 From Kyle Daruwalla : Shashi you are cutting out

12:30:44	 From Shashi Gowda : I'm gonna move to a different place give me a min

12:41:35	 From Ari Katz : to everyone who just joined, welcome! We're talking about parallelism options for Julia's fastai package

12:56:19	 From Shashi Gowda : http://shashi.biz/FileTrees.jl/

12:57:42	 From Michael Pieler : Because we just discussing parallel data loading: the PyTorch data loader has an option to do that: https://pytorch.org/docs/stable/data.html#single-and-multi-process-data-loading

12:58:20	 From Brian Chen : https://github.com/lorenzoh/DataLoaders.jl

12:59:40	 From Neil Rhodes : I’ve got to go. Will try to find something missing and send a PR.

12:59:50	 From Kyle Daruwalla : Thanks Neil

12:59:53	 From Ari Katz : thanks for joining 