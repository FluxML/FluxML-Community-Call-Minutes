# Notes on 27 October 2020

## Agenda

- Governance overview ([Zulip link][1])
- State of ONNX.jl/ONNXmutable.jl ([tracker issue][2])
	- What works, what doesn’t
- Data augmentation ([tracker issue][3])
- Release of FluxTraining.jl v0.1.0
	- Move to FluxML org?
- Pruning methods ([tracker issue][4])
- Triage
	- [https://github.com/FluxML/ML-Coordination-Tracker/issues/9][5]
	- [https://github.com/FluxML/ML-Coordination-Tracker/issues/12][6]
	- Can we create an org-wide triage project?
		- This would remove the need for "forward pointer" issues

## Minutes

**Video link:** [https://youtu.be/TIpA2Y7hg3M][7]

*Note: the recording started a few minutes late.*

### Governance overview

Kyle gave a summary of the Zulip topic. The main point is: *don’t make direct commits to released repos. Use reviewed PRs instead.*

### State of ONNX

[ONNXmutable.jl][8] works reliably for vision models. Other models should work, but they have not been tested. It used to be based on ONNX.jl, but Peter created [BaseONNX.jl][9] which has been used as the base for ONNXmutable.jl on a recent branch.

The issue of non-standard operations was mentioned. The ONNX spec is simple and focused on low-level operations. Something like NaiveNASflux.jl can be used to build more complex ops from low-level ops (e.g. `BasicBlock` from PyTorch’s ResNet implementations). It is currently rough around the edges, so a more generic model building API would be useful.

Some future improvements:
- Current design is monolithic
	- Break it up into smaller packages to encourage contributions
- NNlib interface: [https://github.com/FluxML/NNlib.jl/issues/224][10]
	- Make ONNXmutable understand this set of operations specifically

### User stories

The goal is to guide development, test current packages (e.g. ONNXmutable.jl end-to-end example), and provide an overview of the low-level & high-level parts of the ecosystem. The implementation is code + markdown to describe characteristic programs. Lorenz suggested using [Publish.jl][11] to make these in the tracker repo.

We specifically focused on medical imaging use cases supported by [MONAI][12] in PyTorch. Dale is looking to move his research over to Julia + Flux, and we decided that implementing some MONAI examples would make good user stories. [This example][13] was suggested as a good first step.

### Data augmentation

The goal is to build an interface for general use (i.e. extensible), but we should provide common augmentations in the default package. Application-specific augmentations can be built on the interface (e.g. the augmentations that come from the medical imaging user story above).

Next steps:
- Add docs to [DataAugmentation.jl][14] and have folks review the interface
- Collect list of augmentations in other libraries like fast.ai in design doc

### FluxTraining.jl release

Lorenz is okay with moving to FluxML after the first release. At the same time, we should update FastAI.jl to make use of the first release. *Lorenz requested we review the docs for FluxTraining.jl and file issues/PRs to improve them as needed.*

### Pruning methods

Some gripes from PyTorch:
- Need to pass pruning method a list of tuples of parameter names and parameter that you want to prune
- Model state is “corrupted” by pruning state
	- Forced to “fix” the pruning before checkpointing
	- Pruned checkpoints can’t be loaded into unpruned models
- Generally not a functional approach

Our approach is to provide a functional interface `prune(f) -> f'` where `f'` is usable in the same way as `f`. In other words, pruning should appear like a model transformation and not the addition of implementation state. There are two ideas for how to do this:
- Use a closure around a layer to mask weights/activations based on the pruning method and return the closure. Zygote’s `hook` can be used to apply the same masking to gradients before they are applied. This approach might not be extensible unless we make the closure more explicit by using a `PrunedLayer` wrapper type. That seems closer to PyTorch-land, so the design would need to be carefully done to stop implementation leakage.
- Define a new masked array type that behaves the same as a regular array, except that `getindex` applies masking to the element. Similarly `∇getindex` can be defined to mask the gradients. This approach is more extensible.

Christian also mentioned that NaiveNASflux.jl can perform pruning operations and make optimal decisions using JuMP or CBC solvers. This is an advantage that other frameworks don’t have, so we should look to support both methods from literature as well as “throw the problem to an optimization solver” methods that are possible with Julia. Probably all that needs to be done for NaiveNASflux.jl support is a friendly API for pruning-specific mutations of the model.

### Triage

We discussed both issues, and it seems that in both cases, we should bring up the issue at the ML/AD dev call. If the answer is to wait until X is released, then we can seek how we should resolve these issues for users. It was suggested that the community maintainers could help with lower priority repos like the model-zoo.

## Chat log

12:08:53	 From Dale Black : Could you resend the links into the chat

12:09:14	 From Lorenz Ohly : https://github.com/FluxML/ML-Coordination-Tracker/blob/master/meeting-minutes/2020-10-26.md

12:09:23	 From Lorenz Ohly : The ONNX issue: https://github.com/FluxML/ML-Coordination-Tracker/issues/10

12:18:42	 From Brian Chen : https://github.com/FluxML/NNlib.jl/issues/224

12:23:29	 From Brian Chen : https://github.com/chengchingwen/Transformers.jl

12:25:22	 From Ari Katz : https://docs.fast.ai/tutorial.text

12:25:38	 From Ari Katz : https://docs.fast.ai/tutorial.text#Fine-tuning-a-language-model-on-IMDb

12:27:02	 From Ari Katz : Fast.AI has a high level finetuning interface

12:29:23	 From Dale Black : https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/brats_segmentation_3d.ipynb

12:29:59	 From Ari Katz : https://github.com/FluxML/Flux3D.jl

12:52:53	 From Kyle Daruwalla : https://github.com/FluxML/ML-Coordination-Tracker/issues/9

13:08:49	 From Ari Katz : gotta run 

13:08:50	 From Ari Katz : cya

13:09:05	 From Lorenz Ohly : cya

[1]:	https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/Repo.20access.20and.20organization
[2]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/10
[3]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/11
[4]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/13
[5]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/9
[6]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/12
[7]:	https://youtu.be/TIpA2Y7hg3M
[8]:	https://github.com/DrChainsaw/ONNXmutable.jl
[9]:	https://github.com/opus111/BaseOnnx.jl
[10]:	https://github.com/FluxML/NNlib.jl/issues/224
[11]:	https://github.com/MichaelHatherly/Publish.jl
[12]:	https://monai.io
[13]:	https://github.com/Project-MONAI/tutorials/blob/master/3d_segmentation/brats_segmentation_3d.ipynb
[14]:	https://github.com/lorenzoh/DataAugmentation.jl