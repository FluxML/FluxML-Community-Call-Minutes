# Notes on 27 Jan. 2023

## Agenda

- Review questions outstanding in [NNlib dropout PR](https://github.com/FluxML/Flux.jl/pull/2150)
- Discuss possible GSoC projects for this year

## Minutes

**Video link:** https://youtu.be/FGT7Qe7XWnY

We discussed the outstanding issues for the dropout PR:
- The default RNG issue is that pre-1.7, most people "expected" `rand` with no RNG specified to invoke `Random.GLOBAL_RNG` which only exists on the first thread but `Random.default_rng` (backported pre-1.7) is task-specific (matching 1.7+)
    - The solution is to keep the old behavior which does the "expected" thing by defining a version-specific indirection called `rng_default_value`
- The `Dropout(0.0)` / `Dropout(1.0)` issue is about returning `identity` / `zero` instead of `Dropout`
    - We typically don't do this (except more recently for `Dense(::Diagonal, ...) == Scale`)
    - Our decision was to return the type (i.e. always return `Dropout`) at the Flux/NNlib level in the stack
    - Higher level libraries can optimize this away, and Fluxperimental.jl can have a tool for doing this at the user's discretion

We discussed possible GSoC projects for this year:
- We will remove all previous projects for being completed or stale
- We will have a project for creating a benchmarking tool that can measure performance of typical models, operations, and gradient calls using user-specified package versions across the FluxML-stack
    - This should be callable and configurable from CI
- We will have a project (with Julian as a co-mentor) on writing Julia-native (i.e. KernelAbstraction.jl) kernels for common operations like matmul, normalization, or activations
    - The end goal is to be able to fuse kernels on device (which we normally can only do within vendor-specific kernel boundaries)
    - We will start a HackMD to brainstorm the scope of this project among the possible mentors
    - There are obvious challenges here related to AD breaking fusion
- Potentially these two projects can work off each other so that at the end of the summer, we can use one to benchmark the performance gain/loss of the other (and provide a nice target for AD folks to use moving forward)

### Chat transcript

```
12:08:31	 From Kyle Daruwalla : https://github.com/FluxML/Flux.jl/pull/2150#discussion_r1064789031
12:13:00	 From Kyle Daruwalla : https://github.com/FluxML/Flux.jl/pull/2150#discussion_r1061018737
12:14:03	 From brianc : https://github.com/FluxML/Flux.jl/blob/v0.13.11/src/layers/basic.jl#L185-L186
12:22:51	 From brianc : Sorry, I don't know why my connection is so cantankerous
12:39:53	 From brianc : https://github.com/EnzymeAD/Enzyme.jl/issues/577
12:43:12	 From brianc : https://julialang.org/jsoc/gsoc/flux/
```
