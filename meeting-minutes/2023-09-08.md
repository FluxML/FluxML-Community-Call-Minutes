# Notes on 8 Sept. 2023

## Agenda

1. RNN API design discussion

## Minutes

## RNN API design discussion

Overview of last call:
- Move state into cell itself (vs. only have initial state inside cell)
- Use immutable `y, model = apply(model, x, ...)` interface
- Preserve single time step behavior:
  ```julia
  model = Chain(RNN(100), Dense(100 => 2))
  # gets a single time step output
  y, model = apply(model, x)
  ```
- `Recurrent` wrapper processes all time steps inside a `Chain`
  ```julia
  model = Chain(Recurrent(RNN(100)), Dense(100 => 2))
  # processes all time steps for xs then passes to Dense
  ys, model = apply(model, xs)
  ```
- Can be used to non-recurrent stateful layers too (e.g. `BatchNorm`)

Possible issue: storing the state inside the cell means that the initial state is not stored anywhere:
- Original plan was to modify `Flux.reset!` to take an initializer function as a keyword
- Some approaches to training RNNs learns the initial state
- Solutions:
    - Store the state inside `Recurrent` similar to Fluxperimental.jl#7 but keep it immutable
    - Store the state inside the cell itself as well as the initial state

Notes on `apply`:
- Output order `y, model` vs `model, y` should match Optimisers.jl
- Default is to accept a single input and let layers customize for `Tuple`/`Vararg` alla `Parallel`

Matthew will update Fluxperimental.jl#7 based on discussion. We can start another PR to test out state inside cell version.

Jonas (author of [GradValley.jl](https://github.com/jonas208/GradValley.jl)) joined the call
- Personal project that ended up getting shared
- Possible contributions to Flux include Tracker.jl updates for ChainRules overload generation
- Additional contribution to NNlib to add a LoopVectorization.jl extension
