# Notes on 24 November 2020

## Agenda

- [Model zoo overhaul][1] (now our primary triage)
- Documentation overhaul/sync
- FastAI.jl updates

## Minutes

**Video link:** [https://youtu.be/3g0vjDA0PBU][2]

### Model zoo updates

Our next priority should be updating the model zoo to reflect improvements across the ecosystem. This includes refreshing tutorials to make use of FluxTraining.jl etc. A summary of the proposed stages towards this goal can be found on [this tracker issue][3].

### Structure of networks

One of the sub-issues for the model zoo update is a Metalhead.jl PR that introduces several model architectures that share a “branch + reduce” style layer (e.g. skip connections or inception blocks). This lead to a discussion about non-standard callable structures and their support. Below is a summary of the approaches, and some (not all!) of their advantages/disadvantages.

*Functional approach*
- Other frameworks (e.g. PyTorch) define custom types, like `BasicBlock` or `InceptionBlock`, to define non-standard layers.
- The [Metalhead.jl PR][4] chooses instead to define functions, like `inceptionblock`, that return a Flux model made of only standard components.
- This approach makes it easier to pass pre-trained models into functions that dispatch by layer type. For example, a user might want to replace all the activations in `Conv` layers with `tanh`.
	- This can be achieved by dispatching on `Chain`, `Conv`, and `Any`, if the model only contains standard layers
	- If a non-standard layer is used, then a special dispatch rule must be added (e.g. on `BottleNeck`) to de-sugar it into function calls on the submodules — as is done for standard layers like `Chain`
- With this (solely) approach, it becomes difficult to recognize structures like `Bottleneck` after they have been created.
- For some blocks, like bottleneck, `SkipConnection` can be used. For others, like inception blocks, a (anonymous) closure must be used. Here is a snippet of that in practice:
	```jl
	function inceptionblock(inplanes, out_1x1,
							red_3x3, out_3x3,
							red_5x5, out_5x5, pool_proj)
	  branch1 = Chain(
	             conv_block((1,1), inplanes, out_1x1)...)

	  branch2 = Chain(
	             conv_block((1,1), inplanes, red_3x3)...,
	             conv_block((3,3), red_3x3, out_3x3; pad=1)...)        

	  branch3 = Chain(
	             conv_block((1,1), inplanes, red_5x5)...,
	             conv_block((5,5), red_5x5, out_5x5; pad=2)...) 

	  branch4 = Chain(
	             MaxPool((3, 3), stride=1, pad=1),
	             conv_block((1,1), inplanes, pool_proj)...)

	  return x -> begin
	    y1 = branch1(x)
	    y2 = branch2(x)
	    y3 = branch3(x)
	    y4 = branch4(x)

	    return cat(y1, y2, y3, y4; dims=3)
	  end
	end
	```

*Custom type approach*
- This is the approach used by most frameworks.
- There are two possible implementations that both introduce a new type (e.g `Bottleneck`):
	1. Create a struct that stores the submodules as separate, unique fields (i.e. what PyTorch does).
	2. Create a “wrapper” type that stores a reference to the standard-layer implementation of the structure (i.e. what’s returned by the functional approach). Here, the wrapper type defers to the underlying reference for the forward pass, etc.
- Wrapper types can be standardized in how they wrap the underlying implementation
- Using custom structures can create additional dependencies when serializing the model

*Functor approach*
- Functors are an abstraction for structures like the layers in this discussion (and standard ones like `Chain`).
- Operations like `fmap` allow someone to apply a function to the **parameters** underlying a complex layer like `Chain`.
- This can be extended to allow shallow traversal of a complex layer. So, instead of going down to parameters, we traverse down to “primitive” layers like `Conv`.
- This allows the custom type approach to gain some of the benefits of the functional approach. In other words, functors make it so that it is not unclear how to de-sugar an arbitrary layer.

*`NamedTuple` approach*
- Sometimes, we don’t want to dispatch on a particular structure (e.g. a “branch-reduce” style layer), but on a specific instance of that structure (e.g. a bottleneck). A custom type can allow us to do this.
- Having custom types also documents the model through the type system. Looking at a `Chain` of standard layers only is hard to decipher, but if the sub-layers are labeled “bottleneck,” then a user can recognize their purpose.
- This proposal extends `Chain` to accept `NamedTuple`s. This adds the “naming” that is lost when foregoing a custom type.
- This change adds a lot of extensibility, addresses some of the issues raised, and seems like it should be easy to implement.

*Standard type approach (i.e. `Parallel`)*
- There is a [current PR to Flux.jl][5] that adds a `Parallel` layer. This takes the “branch-reduce” style layer and turns it into a standard implementation. (*note*: that PR is also a good description of the user-facing issues surrounding this discussion)
- Used with the functional approach, this method allows code to recognize and dispatch on the “branch-reduce” structure within a model.
- Used with the `NamedTuple` approach, this method allows `Parallel` layers within a model to be labeled as bottlenecks, etc.
- Having a standard type not only provides static structural information (something that the functor approach can do too), but also how the sub-modules integrate to produce a forward pass. None of the other methods provide this information (except custom types, but they lack the same generality as `Parallel`).
- Alternatively, instead of having a new layer, we can provide a `branch(x reduce_op, layers...; config)` function that executes `layers` on `x` in parallel according to `config` (for speed), then reduces the outputs with `reduce_op`.
	- Since this approach is at execution-time, inserting it into a `Chain` would require a closure like `x -> branch(x, reduce_op, layers...; config)`
	- The `branch` function can be the forward pass for `Parallel`:
		```jl
		(l::Parallel)(x) =
			branch(x, l.op, l.layers...; config = l.config)
		```
- Dhairya asked what the forward pass definition should be. Borrowing from the Flux.jl PR:
	```jl
	struct Parallel{F, T}
	  fs::F
	  op::T
	end

	# init might need to specified depending on l.op
	# init might need to be a field in Parallel
	(l::Parallel)(x::AbstractArray) =
		mapreduce(f -> f(x), l.op, l.fs; init = zero(x))
	```

In the end, there appeared to be some agreement that the `Parallel` approach had enough benefits to implement regardless of whether we implement the other approaches as well.

### Model visualization

One application of the discussions above is model visualization. Brian suggested that even with the approaches above, it would still be better to go to ONNX to visualize models.

Since the structural discussion does have some implications for ONNX support, it is worth noting that the functor approach might help translating a Flux model to ONNX.

### Other updates

- Lorenz is adding 3D data support to DataAugmentation.jl based on Dale’s feedback.
- Peter has [a PR][6] to update the FastAI.jl README.md. Feedback in the form of reviews is appreciated.
- Lorenz will set up Publish.jl for our documentation across ecosystem repos.

## Chat log

12:00:18	 From Kyle Daruwalla : Anton are you having trouble connecting?

12:02:44	 From Anton : Hi Kyle, I think my audio is fine (I hear keyboard), visio is good too - i could see you earlier

12:07:06	 From Kyle Daruwalla : https://github.com/FluxML/ML-Coordination-Tracker/blob/master/meeting-minutes/2020-11-23.md

12:07:22	 From Kyle Daruwalla : https://github.com/FluxML/ML-Coordination-Tracker/issues/9

12:08:52	 From Brian Chen : Had a look through yesterday and it seems good to me!

12:28:04	 From Brian Chen : https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html?highlight=sequential#torch.nn.Sequential

12:29:57	 From Ari Katz : that makes senes

12:30:02	 From Ari Katz : sense * 
12:40:57	 From Brian Chen : https://github.com/FluxML/ML-Coordination-Tracker/issues/10

12:46:54	 From Ari Katz : https://www.tensorflow.org/swift/api_docs/Protocols/Layer

13:00:54	 From Ari Katz : so how about a layer interface package? with interfaces that can be overloaded and traits 

13:04:45	 From Dhairya Gandhi : ```julia
`
13:04:47	 From Dhairya Gandhi : struct Parallels{T}
  fs::T
  r
end

function (p::Parallels)(x...)
  mapreduce((f,x) -\> f(x), p.r, zip(p.fs, x))
end

(p::Parallels)(x::AbstractArray) = p(ntuple(_ -\> x, length(p.fs)))

[1]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/9
[2]:	https://youtu.be/3g0vjDA0PBU
[3]:	https://github.com/FluxML/ML-Coordination-Tracker/issues/9
[4]:	https://github.com/FluxML/Metalhead.jl/pull/70#
[5]:	https://github.com/FluxML/Flux.jl/pull/1289#
[6]:	https://github.com/FluxML/FastAI.jl/pull/14#
