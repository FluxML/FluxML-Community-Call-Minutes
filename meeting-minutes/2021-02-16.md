# Notes on 16 February 2021

## Agenda

- Metalhead.jl updates
	- Pre-trained models obtained
	- PR needs some cleaning then ready for review
- MLDatasets.jl updates
	- Merge DLDatasets.jl/FastAI.jl datasets in
	- Refactor MLDatasets.jl API to look more like [FastAI.jl/src/datasets][1]
- MLDataPattern.jl/MLLabelUtils.jl/LearnBase.jl
	- Refactor nearly complete
	- Need to go over testing before submitting PRs
	- Need to go through feature revision
		- e.g. [time series data][2]
- Triage
	- [onehot.md in docs should not have subtitle "Batches"][3]
	- [RNN layer to skip certain time steps (like \`Masking\` layer in keras)][4]
	- [accumulate gradient with the new gradient API?][5]
	- [generic\_matmul! hit in \`back!\` because type-promotion in activation function][6]

[1]:	https://github.com/lorenzoh/FastAI.jl/tree/master/src/datasets
[2]:	https://julialang.zulipchat.com/#narrow/stream/238249-machine-learning/topic/preprocessing.20data.20MLDataPattern
[3]:	https://github.com/FluxML/Flux.jl/issues/510 "onehot.md in docs should not have subtitle "Batches""
[4]:	https://github.com/FluxML/Flux.jl/issues/644 "RNN layer to skip certain time steps (like `Masking` layer in keras)"
[5]:	https://github.com/FluxML/Flux.jl/issues/707 "accumulate gradient with the new gradient API?"
[6]:	https://github.com/FluxML/Flux.jl/issues/613 "generic_matmul! hit in `back!` because type-promotion in activation function"