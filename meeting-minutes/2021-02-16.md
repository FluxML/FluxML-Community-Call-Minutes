# Notes on 16 February 2021

## Agenda

- Metalhead.jl updates
	- Pre-trained models obtained
	- PR needs some cleaning then ready for review
- MLDatasets.jl updates
	- Merge DLDatasets.jl/FastAI.jl datasets in
	- Refactor MLDatasets.jl API to look more like [FastAI.jl/src/datasets][1]
- MLDataPattern.jl/MLLabelUtils.jl/LearnBase.jl
	- Refactor nearly complete
	- Need to go over testing before submitting PRs
	- Need to go through feature revision
		- e.g. [time series data][2]
- Triage
	- [onehot.md in docs should not have subtitle "Batches"][3]
	- [RNN layer to skip certain time steps (like \`Masking\` layer in keras)][4]
	- [accumulate gradient with the new gradient API?][5]
	- [generic\_matmul! hit in \`back!\` because type-promotion in activation function][6]

## Minutes

**Video link:** [https://youtu.be/snUYGNQI9L8][7]

### Metalhead.jl updates

Pre-trained model weights are transferred to the following models:
- `vgg19`
- `resnet50`
- `googlenet`
- `squeezenet`
- `densenet121`

Dhairya will host weight files on Git LFS or Julia Computing servers. PR will be cleaned up to provide these weights through Pkg artifacts system. Other models will receive pre-trained weights progressively (should be a nice way to test our training pipelines).

### MLDatasets.jl updates

DLDatasets.jl/FastAI.jl datasets will move to MLDatasets.jl. This will be a cold fusion for now (just plain copy, not refactor).

The long term goal is to refactor the MLDatasets.jl to match FastAI.jl datasets. It should be a three tier API:
1. _Low tier_: LearnBase.jl’s `getobs` and `nobs`
2. _Mid tier_: Standard storage formats built on top of the low tier (e.g. a FileTrees.jl data container where you just specify the disk locations)
3. _High tier_: Standard datasets like MNIST built on top of the mid tier

The API will also provide generic utilities to transform data containers (e.g. grouping, splitting, concatenating, etc.). There will also be added support for tabular data in the mid-tier.

### MLDataPattern.jl updates

Slow moving process to refactor the codebase. The current status:
- _LearnBase.jl_: complete (PR in place)
- _MLLabelUtils.jl_: refactor complete, almost all test cases passing (PR incoming)
- _MLDataPattern.jl_: `gettarget` refactor left post-MLLabelUtils.jl changes, test cases needed to be refactored (PR in place)

Need to also refactor community packages to use `obsdim` keyword. The default observation dimension can be specified for a data container by overloading `default_obsdim`.

### Time series data

Current RNN data is a vector of time steps to represent sequences. CuDNN doesn’t like this format. Vector of time steps seems like it has clarity/pedagogical value, but it doesn’t have much beyond this.

RNN docs have been [updated][8]. This should help the confusion around the current API.

MLDataPattern.jl also has this problem when `slidingwindow` is applied. We want to agree on the vanilla Flux API, then make MLDataPattern.jl do the behavior that works best with Flux.

### Aside on Optimisers.jl

Everyone wants immutable optimizers. There is currently a discussion surrounding the deprecation path. Should we have mutable optimizers with explicit state as an intermediate point? How much performance will be lost (if any) due to extra allocations with immutability? These are questions that we plan on answering by testing the current Optimisers#master branch against the current Flux.

### Triage

#### [onehot.md in docs should not have subtitle "Batches"][9]

Kyle will submit PR to fix.

#### [RNN layer to skip certain time steps (like \`Masking\` layer in keras)][10]

Brian will update the issue. Solution suggested (skipping time steps that are missing) does not work for multi-variate features where only certain features are missing per time step.

Kyle was planning a MaskedArrays.jl for pruning. We can also use the same mechanism for this.

#### [accumulate gradient with the new gradient API?][11]

Seems like a good idea to support gradient algebra. The agreed upon approach appears to be implementing `map`/`map!` on `Params`/`Grads`. Then extend broadcasting of `+` etc. to route to `map`/`map!`. Kyle will update the issue.

#### [generic\_matmul! hit in \`back!\` because type-promotion in activation function][12]

Everyone wants to address the issue, but we are hesitant to shadow `Base.:*`. It would be preferable to do this in Base or to use a utility function that runs a performance check on the model.

[1]:	https://github.com/lorenzoh/FastAI.jl/tree/master/src/datasets
[2]:	https://julialang.zulipchat.com/#narrow/stream/238249-machine-learning/topic/preprocessing.20data.20MLDataPattern
[3]:	https://github.com/FluxML/Flux.jl/issues/510 "onehot.md in docs should not have subtitle "Batches""
[4]:	https://github.com/FluxML/Flux.jl/issues/644 "RNN layer to skip certain time steps (like `Masking` layer in keras)"
[5]:	https://github.com/FluxML/Flux.jl/issues/707 "accumulate gradient with the new gradient API?"
[6]:	https://github.com/FluxML/Flux.jl/issues/613 "generic_matmul! hit in `back!` because type-promotion in activation function"
[7]:	https://youtu.be/snUYGNQI9L8
[8]:	https://github.com/FluxML/Flux.jl/pull/1428
[9]:	https://github.com/FluxML/Flux.jl/issues/510 "onehot.md in docs should not have subtitle "Batches""
[10]:	https://github.com/FluxML/Flux.jl/issues/644 "RNN layer to skip certain time steps (like `Masking` layer in keras)"
[11]:	https://github.com/FluxML/Flux.jl/issues/707 "accumulate gradient with the new gradient API?"
[12]:	https://github.com/FluxML/Flux.jl/issues/613 "generic_matmul! hit in `back!` because type-promotion in activation function"