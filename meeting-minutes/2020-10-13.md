# Notes on 13 October 2020

## Agenda

- FluxTraining.jl roadmap
	- FastAI.jl roadmap
	- Loggers?
- Data loader
	```jl
	data = MLDatasets.DataLoader{CIFAR10}()

	# Example 1
	loader = data |>
			 x -> eachbatch(x, 32) |>
	         shuffleobs |>
	         parallelobs |>
			 CuIterator
	for (x, y) in loader
		# do stuff
	end

	# Example 2
	loader = CuIterator(parallelobs(shuffleobs(eachbatch(data, 32))))
	for (x, y) in loader
		# do stuff
	end

	# vs PyTorch-like syntax
	# what we might want for FastAI.jl
	# already designed by github/@lorenzoh
	loader = DataLoader(data; batch_size = 32,
							  parallel = true)
	for (x, y) in CuIterator(shuffleobs(loader))
		# do something
	end
	```
	- DataSets.jl support
- Data augmentation

## Minutes

**Video Link**: [https://youtu.be/rHczvu6DJ7s][1]

_Note: we had some new participants who asked for an ecosystem overview; this is excluded from the minutes, but you can check out the video recording for the summary._

### FluxTraining.jl Roadmap

**Current state:** (this excludes features that exist in the repo but are still open discussions; e.g. data augmentation)
- Working data loaders (via [lorenzoh/DataLoaders.jl][2], soon to be moved to MLDataPattern.jl)
- Working training loop
- Working callback system built upon LightGraphs.jl
	- Sequential execution

**Planned roadmap:** (broad priorities for our next iteration)
- ONNX support by reviving ONNX.jl and/or ONNXmutable.jl (relevant [Zulip thread][3])
- Designing a data augmentation pipeline (see discussion below)
- Moving to an external DAG package for callbacks

Additionally, we need to prove our current state. Peter suggested taking a popular fast.ai example and writing it in FluxTraining.jl. Brian also suggested making sure [this example][4] from the repo works.

### Abstract DAG

Julian offered to assist with Dagger.jl and DAG API support.

Dagger will soon be able to query the hardware and OS to dynamically adjust load.
- From the last meeting, there were concerns about the composability of Dagger.jl with a larger program that uses “X” multi-processing/threading library. Shashi suggested this was a scheduling issue, since the scheduler is unaware of the global schedule state. This should address that for the most part.

### Data loader

Kyle gave a summary of the data loading design (see Agenda above). Notably the `MLDatasets.DataLoader{T}` wrapper uses multiple dispatch to reroute `getobs` calls to the appropriate methods like `traintensor`. This separation means that we can later support a refactor of MLDatasets.jl to [DataSets.jl][5] easily.

How does the data loader pipeline compose with the DAG in FluxTraining.jl?
- The data loading pipeline is a set of lazy wrappers around a single high-latency task — reading from disk. It is only important that the pipeline runs on a separate worker from the main task, so using a DAG scheduler for individual nodes in the loading pipeline is not necessary.
- Treat the data loading pipeline as a source node in the DAG.
- Just avoid `parallelobs` when using with DAG to allow Dagger.jl to execute the data source node on a separate worker using its scheduler.

### Data augmentation

Current approaches:
- _[Augmentor.jl][6]_: only for images but could serve as good inspiration
- _[lorenzoh/DataAugmentation.jl][7]_: cost/benefit unknown; maybe Lorenz can comment?

Data augmentation is described naturally as a pipeline. So, it would make sense to build upon an existing pipeline system.
- MLJ is an option, but it seems like MLJBase wasn’t made to be a generic platform (harder to integrate)
- [Transducers.jl/SplittablesBase.jl][8] seems like a good option



### Triage

This should be a priority for general ecosystem improvement. Peter has some good first RNN-related issues to test drive the triage pipeline.

The overall pipeline:
1. Issues filed on Flux/Zygote/whatever
2. Stale issues are identified, and issues w/ forward pointers are filed on ML tracker
3. Issue in tracker is labeled as needing triage
4. Prior to our meeting, we select some issues needing triage for review
5. Spend last 10 minutes of meeting reviewing issues, and select issues to alert core devs about
6. Alert core team during ML/AD dev call
7. Mark issue as triaged, then resolved once fix is merged

### Chat Log

12:03:40	 From Kyle Daruwalla : https://github.com/FluxML/ML-Coordination-Tracker/blob/master/meeting-minutes/2020-10-13.md

12:14:12	 From Brian Chen : https://github.com/lorenzoh/FluxTraining.jl/blob/master/examples/FluxTraining.jl-demo.ipynb

12:15:36	 From Brian Chen : https://github.com/lorenzoh/DeepLearningTasks.jl

12:18:06	 From Kyle Daruwalla : https://fluxml.ai

12:20:38	 From Julian Samaroo : Well... AMDGPU isn't supported in Flux *yet* :)

12:21:21	 From Brian Chen : https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination

12:21:35	 From Brian Chen : ^ this is where we usually hang out

12:21:41	 From Kyle Daruwalla : https://julialang.org/community/

12:22:14	 From Ari Katz : https://julialang.org/learning/

12:25:27	 From Huda Nassar : @aaron — this might be useful to you if you are interested in contributing to the Julia ecosystem in general not just flux… https://juliacommunity.github.io/your-first-julia-pr/

12:25:52	 From Brian Chen : See https://github.com/FluxML/ML-Coordination-Tracker/blob/master/meeting-minutes/2020-09-29.md#design-of-dag-system for a summary of the DAG discussion from last meeting.

12:27:53	 From Julian Samaroo : Btw Dagger currently has some prototype GPU support via DaggerGPU, and I'd be happy to help with using Dagger or making changes to it to suit the needs of the Flux community.

12:29:42	 From Brian Chen : https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/AbstractDAG.20design

12:35:17	 From Ari Katz : https://github.com/JuliaParallel/Dagger.jl/pulse

12:39:38	 From Kyle Daruwalla : https://github.com/lorenzoh/DataAugmentation.jl

12:40:43	 From Brian Chen :   https://github.com/IBM/AutoMLPipeline.jl

12:49:15	 From Brian Chen : https://github.com/JuliaFolds/Transducers.jl

12:56:49	 From Huda Nassar : I have to go in a minute — but a quick question: is there a roadmap written anywhere about the stuff that were discussed today + future items?

12:57:25	 From Ari Katz : yup I'll post a link 

12:57:31	 From Huda Nassar : Great, thanks!

12:57:53	 From Brian Chen : https://github.com/FluxML/ML-Coordination-Tracker/tree/master/meeting-minutes

13:06:27	 From Brian Chen : @Ari see https://github.com/FluxML/ML-Coordination-Tracker/issues/9 for an issue w/ many "forward pointers"

13:06:47	 From Ari Katz : thanks

[1]:	https://youtu.be/rHczvu6DJ7s
[2]:	https://github.com/lorenzoh/DataLoaders.jl
[3]:	https://julialang.zulipchat.com/#narrow/stream/237432-ml-ecosystem-coordination/topic/ONNX
[4]:	https://github.com/lorenzoh/FluxTraining.jl/blob/master/examples/FluxTraining.jl-demo.ipynb
[5]:	https://github.com/JuliaComputing/DataSets.jl
[6]:	https://github.com/Evizero/Augmentor.jl
[7]:	https://github.com/lorenzoh/DataAugmentation.jl
[8]:	https://github.com/JuliaFolds/Transducers.jl
